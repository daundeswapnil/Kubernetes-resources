Introduction
Kubernetes (K8s) is an open-source container orchestration system for automating deployment, scaling, and management of containerized applications.
On very high level Kubernetes provides a set of dynamically scalable hosts for running workloads using containers and uses a set of management hosts called masters for providing an API for managing the entire container infrastructure. 
The workloads could include long-running services, batch jobs and container host specific daemons.

All the container hosts are connected together using an "overlay network" for providing container-to-container routing. Applications deployed on Kubernetes are dynamically discoverable within the cluster network and can be exposed to the external networks using traditional "load balancers". 
The state of the cluster manager is stored on a "highly distributed key/value store etcd" which runs within the master instances.

Kubernetes scheduler will always make sure that each application component is health checked, provided high availability, when the number of replicas is set to more than one each instance is scheduled in multiple hosts, and if one of those hosts becomes unavailable all the containers which were running in that host are scheduled in any of the remaining hosts. 

One of the fascinating capabilities Kubernetes offers is two level autoscaling.
1. It provides the ability to autoscale containers using a resource called "Horizontal Pod Autoscaler" which watches the resource consumption and scale the number of containers needed accordingly. 
2. It can scale the container cluster itself by "adding and removing hosts" depending on the resource requirements.


Nodes
A node is the smallest unit of computing hardware in Kubernetes.
It is a representation of a single machine in your cluster.
we can instead simply view each machine as a set of CPU and RAM resources that can be utilized.
In this way, any machine can substitute any other machine in a Kubernetes cluster.


The Cluster
In Kubernetes, nodes pool together their resources to form a more powerful machine.
When you deploy programs onto the cluster, it intelligently handles distributing work to the individual nodes for you.
If any nodes are added or removed, the cluster will shift around work as necessary. It shouldn’t matter to the program, or the programmer, which individual machines are actually running the code.


Persistent Volumes
To store data permanently, Kubernetes uses Persistent Volumes.
While the CPU and RAM resources of all nodes are effectively pooled and managed by the cluster, persistent file storage is not.
Instead, local or cloud drives can be attached to the cluster as a Persistent Volume. This can be thought of as plugging an external hard drive in to the cluster.
Persistent Volumes provide a file system that can be mounted to the cluster, without being associated with any particular node.
Applications that require persisting data on the filesystem may use volumes for mounting storage devices to ephemeral containers similar to how volumes are used with VMs.

A PVC defines the disk size, disk type (ReadWriteOnce, ReadOnlyMany, ReadWriteMany) and dynamically links a storage device to a volume defined against a pod. 
The binding process can either be done in a static way using PVs or dynamically be using a persistent storage provider. 
In both approaches, a volume will get linked to a PV one to one and depend on the configuration given data will be preserved even if the pods get terminated.

According to the disk type used multiple pods will be able to connect to the same disk and read/write.
Disks that support "ReadWriteOnce" will only be able to connect to a single pod and will not be able to share among multiple pods at the same time. However, disks that support "ReadOnlyMany" will be able to share among multiple pods at the same time in read only mode. In contrast, as the name implies disks with "ReadWriteMany" support can be connected to multiple pods for sharing data in read and write mode. 

Kubernetes provides a "collection of volume plugins" for supporting storage services available on public cloud platforms such as AWS EBS, GCE Persistent Disk, Azure File, Azure Disk and many other well-known storage systems such as NFS, Glusterfs, Cinder, etc.


Containers
Programs running on Kubernetes are packaged as Linux containers.
Containerization allows you to create self-contained Linux execution environments.
Any program and all its dependencies can be bundled up into a single file.
Multiple programs can be added into a single container, but you should limit yourself to one process per container if at all possible.
It’s better to have many small containers than one large one.
If each container has a tight focus, updates are easier to deploy and issues are easier to diagnose.


Pods
Kubernetes doesn’t run containers directly; instead it wraps one or more containers into a higher-level structure called a pod.
Each pod allows sharing the file system, network interfaces, operating system users, etc among the containers using Linux namespaces, cgroups, and other kernel features.
Containers can easily communicate with other containers in the same pod as though they were on the same machine while maintaining a degree of isolation from others.
Pods are used as the unit of replication in Kubernetes. 
If your application becomes too popular and a single pod instance can’t carry the load, Kubernetes can be configured to deploy new replicas of your pod to the cluster as necessary. 
Even when not under heavy load, it is standard to have multiple copies of a pod running at any time in a production system to allow load balancing and failure resistance.
Pods can hold multiple containers, but you should limit yourself when possible. Because pods are scaled up and down as a unit, all containers in a pod must scale together, regardless of their individual needs. 
This leads to wasted resources and an expensive bill. To resolve this, pods should remain as small as possible, typically holding only a main process and its tightly-coupled helper containers (these helper containers are typically referred to as “side-cars”). 


ReplicaSet 
A ReplicaSet can be considered as a YAML or a JSON based metadata file which defines the container images, ports, the number of replicas, activation health checks, liveness health checks, environment variables, volume mounts, security rules, etc required for creating and managing the containers.
The ReplicaSets can be managed by another high level resource called Deployments


Deployments
Although pods are the basic unit of computation in Kubernetes, they are not typically directly launched on a cluster. Instead, pods are usually managed by one more layer of abstraction: the deployment.
A deployment’s primary purpose is to declare how many replicas of a pod should be running at a time. 
When a deployment is added to the cluster, it will automatically spin up the requested number of pods, and then monitor them. If a pod dies, the deployment will automatically re-create it.
Using a deployment, you don’t have to deal with pods manually. You can just declare the desired state of the system, and it will be managed for you automatically.
Deployments for providing features for rolling out updates and handling their rollbacks.

A containerized application can be deployed on Kubernetes using a deployment definition by executing a simple CLI command as follows:
kubectl run <application-name> --image=<container-image> --port=<port-no>
Once the above CLI command is executed, it will create a deployment definition, a replica set and a pod using the given container image add a selector label using the application name.


DaemonSets
Kubernetes provides a resource called "DaemonSets" for running a copy of a pod in each Kubernetes node as a daemon. 

Some of the use cases of DaemonSets are as follows:
1. A cluster storage daemon such as glusterd , ceph to be deployed on each node for providing persistence storage.
2. A node monitoring daemon such as Prometheus Node Exporter to be run on every node for monitoring the container hosts.
3. A log collection daemon such as fluentd or logstash to be run on every node for collecting container and Kubernetes component logs.
4. An ingress controller pod to be run on a collection of nodes for providing external routing.


StatefulSets
StatefulSets are similar to ReplicaSets except that it provides the ability to handle the startup sequence of pods, uniquely identify each pod for preserving its state while providing following characteristics:
Stable, unique network identifiers.
Stable, persistent storage.
Ordered, graceful deployment and scaling.
Ordered, graceful deletion and termination.
Ordered, automated rolling updates

Deploying databases on container platforms for production usage would be a slightly difficult task than deploying applications due to their requirements for clustering, point to point connections, replication, shading, managing backups, etc. As mentioned previously StatefulSets have been designed specifically for supporting such complex requirements and there are a couple of options for running PostgreSQL, and MongoDB clusters on Kubernetes.


Running Background Jobs
In addition to ReplicaSets and StatefulSets Kubernetes provides two additional controllers for running workloads in the background called Jobs and CronJobs. 
The difference between Jobs and CronJobs is that "Jobs execute once and terminates" whereas "CronJobs get executed periodically by a given time interval" similar to standard Linux cron jobs.


Service
One of the key features of Kubernetes is its "service discovery and internal routing model" provided using "SkyDNS and layer 4 virtual IP" based routing system. These features provide internal routing for application requests using services. 
A set of pods created via a replica set can be load balanced using a service within the cluster network. The services get connected to pods using selector labels. Each service will get assigned a unique IP address, a hostname derived from its name and route requests among the pods in round robin manner.

The services will even provide IP-hash based routing mechanism for applications which may require session affinity. 
A service can define a collection of ports and the properties defined for the given service will apply to all the ports in the same way. Therefore, in a scenario where session affinity is only needed for a given port where all the other ports required to use round robin based routing, multiple services may need to be used.


kube-proxy
Kubernetes services have been implemented using a component called kube-proxy. A kube-proxy instance runs in each node and provides three proxy modes: Userspace, iptables and IPVS. The current default is iptables.

In the first proxy mode: userspace, kube-proxy itself will act as a proxy server and delegate requests accepted by an iptable rule to the backend pods. In this mode kube-proxy will operate in the userspace and will add an additional hop to the message flow. 

In the second proxy mode: iptables, the kube-proxy will create a collection of iptable rules for forwarding incoming requests from the clients directly to the ports of backend pods on the network layer without adding an additional hop in the middle. This proxy mode is much faster than the first mode because of operating in the kernel space and not adding an additional proxy server in the middle.

The third proxy mode was added in Kubernetes v1.8 which is much similar to the second proxy mode and it makes use of an IPVS based virtual server for routing requests without using iptable rules. 
IPVS is a transport layer load balancing feature which is available in the Linux kernel based on Netfilter and provides a collection of load balancing algorithms. 
The main reason for using IPVS over iptables is the performance overhead of syncing proxy rules when using iptables. 
When thousands of services are created, updating iptable rules takes a considerable amount of time compared to few milliseconds with IPVS. Moreover, IPVS uses a hash table for looking up the proxy rules over sequential scans with iptables.

Kubernetes "services" can be "exposed" to the external networks in two main ways. 
1.By using node ports by "exposing dynamic ports" on the nodes that forward traffic to the service ports. 
2. By using a load balancer configured via an ingress controller which can delegate requests to the services by connecting to the same overlay network.


Ingress controller
 An ingress controller is a background process which may run in a container which listens to the Kubernetes API, dynamically configure and reloads a given load balancer according to a given set of ingresses. An ingress defines the routing rules based on hostnames and context paths using services.
 
 kubectl expose deployment <application-name> --type=LoadBalancer --name=<service-name>
The above command will create a service of load balancer type and map it to the pods using the same selector label created when the pods were created.


Ingress
you can create a cluster of nodes, and launch deployments of pods onto the cluster. There is one last problem to solve, however: allowing external traffic to your application.
By default, Kubernetes provides isolation between pods and the outside world. If you want to communicate with a service running in a pod, you have to open up a channel for communication. This is referred to as ingress.
There are multiple ways to add ingress to your cluster. The most common ways are by adding either an Ingress controller, or a LoadBalancer. The exact tradeoffs between these two options are out of scope for this post, but you must be aware that ingress is something you need to handle before you can experiment with Kubernetes.


ConfigMaps (Configurations Management)
Containers generally use environment variables for parameterizing their runtime configurations. However, typical enterprise applications use a considerable amount of configuration files for providing static configurations required for a given deployment. 
Kubernetes provides a fabulous way of managing such configuration files using a simple resource called "ConfigMaps" without bundling them into the container images.

ConfigMaps can be created using directories, files or literal values using following CLI command:
kubectl create configmap <map-name> <data-source>

Once a ConfigMap is created, it can be mounted to a pod using a volume mount. With this loosely coupled architecture, configurations of an already running system can be updated seamlessly just by updating the relevant ConfigMap and executing a rolling update process.
Currently ConfigMaps do not support "nested folders", therefore if there are configuration files available in a nested directory structure of the application, a ConfigMap would need to be created for each directory level.


Secrets (Credentials Management)
K8s use Secrets for managing sensitive information such as passwords, OAuth tokens, and ssh keys. Otherwise updating that information on an already running system might require rebuilding the container images.
A secret can be created for managing basic auth credentials using the following way:
# write credentials to two files
$ echo -n 'admin' > ./username.txt
$ echo -n '1f2d1e2e67df' > ./password.txt
# create a secret
$ kubectl create secret generic app-credentials --from-file=./username.txt --from-file=./password.txt

Once a secret is created, it can be read by a pod either using environment variables or volume mounts.


blue/green deployment
This is another invaluable feature of Kubernetes which allows applications to "seamlessly roll out security updates and backward compatible" changes without much effort. 
If the changes are not backward compatible, a manual blue/green deployment might need to be executed using a separate deployment definition.
This approach allows a rollout to be executed for updating a container image using a simple CLI command:
$ kubectl set image deployment/<application-name> <container-name>=<container-image-name>:<new-version>

Once a rollout is executed, the status of the rollout process can be checked as follows:
$ kubectl rollout status deployment/<application-name>
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment "<application-name>" successfully rolled out

Using the same CLI command kubectl set image deployment an update can be rolled back to a previous state.


Autoscaling
Kubernetes allows pods to be manually scaled either using ReplicaSets or Deployments. 
The following CLI command can be used for this purpose:
kubectl scale --replicas=<desired-instance-count> deployment/<application-name>

functionality can be extended by adding another resource called Horizontal Pod Autoscaler (HPA) against a deployment for dynamically scaling the pods based on their actual resource usage. 
The HPA will monitor the resource usage of each pod via the "resource metrics API" and inform the deployment to change the replica count of the ReplicaSet accordingly. 
Kubernetes uses an "upscale delay and a downscale delay" for avoiding thrashing which could occur due to frequent resource usage fluctuations in some situations. Currently, HPA only provides support for scaling based on CPU usage. If needed custom metrics can also be plugged in via the "Custom Metrics API" depending on the nature of the application.

Helm (Package Management)
This allows Kubernetes resources such as deployments, services, configmaps, ingresses, etc to be templated and packaged using a resource called chart and allow them to be configured at the installation time using input parameters.
More importantly, it allows existing charts to be reused when implementing installation packages using dependencies. 
Helm repositories can be hosted in public and private cloud environments for managing application charts. 
Helm provides a CLI for installing applications from a given Helm repository into a selected Kubernetes environment.


Setting Up a Cluster
kubectl
The tool you use when setting up a Kubernetes environment is the kubectl command. 
This command allows you to interact with the Kubernetes API. 
kubectl is used to manage the resources (create, update, and delete Kubernetes resources) like pods, deployments, and load balancers.
kubectl can’t be used to directly provision the nodes or clusters your pods are run on. 
This is because Kubernetes was designed to be platform agnostic. 
Kubernetes doesn’t know or care where it is running, so there is no built in way for it to communicate with your chosen cloud provider to rent nodes on your behalf.


#####################################################################################################

Deploying An App

YAML: Declarative Infrastructure
There are two ways to add resources to Kubernetes: interactively through the command line using kubectl add,
and declaratively, by defining resources in YAML files.
YAML is the way to go when you want to build something maintainable. 
By writing all of your Kubernetes resources into YAML files, you can record the entire state of your cluster in a set of easily maintainable files, which can be version-controlled and managed like any other part of your system. 
In this way, all the instructions needed to host your service can be saved right alongside the code itself.


Adding a Pod : 

1 apiVersion: v1           // this resource is defined in v1 of the Kubernetes API  
2 kind: Pod                // type of resource we are creating is a pod
3 metadata:                /*   3-8 the properties of our pod. In this case, the pod is unoriginally named “gitea-pod”,
                                and it contains a single container we’re calling “gitea-container”. */
4    name: gitea-pod
5 spec:
6    containers:
7      - name: gitea-container
8        image: gitea/gitea:1.4      // container image we want to run


 apply resource to our cluster
 kubectl apply -f gitea.yaml

 get all pods
 kubectl get pods

 to get more information about pod
 kubectl logs -f gitea-pod

 removes all resources defined in the YAML file from the cluster
 kubectl delete -f gitea.yaml

NOTE: there is now a server running inside the container on our cluster! Unfortunately,
      we won’t be able to access it until we start opening up ingress channels 


Deployment :
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: gitea-deployment
spec:
  replicas: 1        // number of copies of the pods we want running
  selector:
    matchLabels:
      app: gitea     // deployment can always keep track of which pods it manages by searching
  template:
    metadata:
      labels:
        app: gitea     //  all pods created by this deployment  if the deployment ever needs to retrieve the list of all pods that it created
    spec:
      containers:
      - name: gitea-container
        image: gitea/gitea:1.4

we are really defining two different objects here:
deployment itself (lines 1–9), and the template of the pod it is managing (lines 10–17).

Labels
Labels are simply user-defined key-value stores associated with Kubernetes resources.

Selectors
Selectors are used retrieve the resources that match a given label query.

apply resource to our cluster
kubectl apply -f gitea.yaml

get information about the deployment 
kubectl get deployments

To test to make sure everything’s working, try deleting the pod with kubectl delete pod <pod_name>. 
You should quickly see a new one pop back in it’s place. That’s the magic of deployments!

################################################################################################

Networking Basics

Opening Container Ports
By default, pods are essentially isolated from the rest of the world. 
In order to route traffic to our application, we need to open the set of ports we plan to use for the container.

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: gitea-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gitea
  template:
    metadata:
      labels:
        app: gitea
    spec:
      containers:
      - name: gitea-container
        image: gitea/gitea:1.4
        ports:                                      #+
        - containerPort: 3000                       #+
          name: http                                #+
        - containerPort: 22                         #+
          name: ssh 

Apply the updated file to the cluster:
 kubectl apply -f gitea.yaml

run kubectl describe deployment to see our newly opened ports listed in the deployment summary
kubectl describe deployment | grep Ports

Debugging with Port Forward
The ports on our container should now be open, but we still need a way to communicate with the pod in the cluster. 
For debugging purposes, we can attach to our pod using kubectl port-forward

# grab the name of your active pod
$ PODNAME=$(kubectl get pods --output=template \
     --template="{{with index .items 0}}{{.metadata.name}}{{end}}")
# open a port-forward session to the pod
$ kubectl port-forward $PODNAME 3000:3000

Now, kubectl will forward all connections on port 3000 on your local machine into the pod running in the cloud

Creating an External LoadBalancer
   
Liveness and Readiness Probes
The kubelet uses liveness probes to know when to restart a Container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.

liveness-probe.yml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
      
      kubectl create -f liveness-probe.yml

Kubernetes Service Objects
In kubernetes service is an object that is logically mapped to pods based on labels.
Service can be single point of contact like loadbalancer for set of pods.
By default Pod is not expose to the internet or outside of the cluster, by using service Pod is exposed to internet.
Any Pod create in future is added to service dynamicaly if pod label is matching with Service lable selector.

service.yml
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: nodeapp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: NodePort
  
kubectl create -f service.yml
  
POD VOLUMES :  
pod-volumes.yml
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
    
kubectl create -f pod-volumes.yml    



